{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f061ca43",
   "metadata": {},
   "source": [
    "### Токенезация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "804b0cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Николай\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d1d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    }
   ],
   "source": [
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "tokens = word_tokenize(data.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9351092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all work and no play makes jack a dull boy, all work and no play']\n"
     ]
    }
   ],
   "source": [
    "tokens = sent_tokenize(data.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd8235",
   "metadata": {},
   "source": [
    "### Razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0772cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize, sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6dc3aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 13, 'Кружка-термос'),\n",
       " Substring(14, 16, 'на'),\n",
       " Substring(17, 20, '0.5'),\n",
       " Substring(20, 21, 'л'),\n",
       " Substring(22, 23, '('),\n",
       " Substring(23, 28, '50/64'),\n",
       " Substring(29, 32, 'см³'),\n",
       " Substring(32, 33, ','),\n",
       " Substring(34, 37, '516'),\n",
       " Substring(37, 38, ';'),\n",
       " Substring(38, 41, '...'),\n",
       " Substring(41, 42, ')')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Кружка-термос на 0.5л (50/64 см³, 516;...)'\n",
    "list(tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46014e41",
   "metadata": {},
   "source": [
    "## re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ff6760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['super', 'c', 'a', 'a', 'c', 'a', 'c']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "word = 'supercalifragilisticexpialidocious'\n",
    "re.findall('[abc]|up|super', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe87c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['49', '432', '312']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d{1,3}', 'These are some numbers: 49 and 432312')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209740c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'can', 'play', 'football']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Удаляем диапазон (заменяем на пробелы)\n",
    "re.sub('[^A-z]',' ','I 123 can 45 play 67 football').split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2bb20e",
   "metadata": {},
   "source": [
    "### N-grams - это просто окна на словах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ada91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all',), ('work',), ('and',), ('no',), ('play',)]\n",
      "[('all', 'work'), ('work', 'and'), ('and', 'no'), ('no', 'play'), ('play', 'makes')]\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(data.lower())\n",
    "unigram = list(nltk.ngrams(tokens, 1))\n",
    "bigram = list(nltk.ngrams(tokens, 2))\n",
    "print(unigram[:5])\n",
    "print(bigram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d84fb649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Популярные униграммы:  [(('all',), 2), (('work',), 2), (('and',), 2), (('no',), 2), (('play',), 2)]\n",
      "Популярные биграммы:  [(('all', 'work'), 2), (('work', 'and'), 2), (('and', 'no'), 2), (('no', 'play'), 2), (('play', 'makes'), 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "print('Популярные униграммы: ', FreqDist(unigram).most_common(5))\n",
    "print('Популярные биграммы: ', FreqDist(bigram).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13604246",
   "metadata": {},
   "source": [
    "### Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3baa9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Николай\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb69077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'does', 'aren', \"isn't\", 'we', 'their', 'who', 'wouldn', 'yourselves', 'his', 'needn', 'than', 'to', 'out', 'am', 'until', 'own', 'do', 'because', 'wasn', \"you'd\", 's', \"aren't\", 'here', 'can', 'such', 'yours', 'had', 'you', 'weren', 'off', 'few', 'hers', 'its', \"you'll\", 'being', 'more', 'be', 'under', \"won't\", 'themselves', 'each', \"mustn't\", 'a', 'while', 'against', 'once', \"hasn't\", \"needn't\", 'these', 'o', 'by', 'should', 'mustn', 'd', 'so', 'mightn', 'ours', 'below', 'hasn', 'as', 'couldn', 'has', 'having', 'hadn', 'and', 'other', 've', 'shouldn', 'all', 'just', 'he', 'myself', 'them', 'with', 'over', 'when', 'why', 'nor', \"doesn't\", 'himself', 'they', 'haven', 'your', 'him', \"should've\", 'any', 'same', 'is', 'was', 'the', 'been', \"hadn't\", \"weren't\", 'she', 'from', 'my', \"couldn't\", 'down', 'on', 'doesn', 'm', \"mightn't\", 'her', 'isn', \"she's\", 'again', 'very', 'it', 'shan', 'above', 'our', 'during', 'yourself', 'most', 'that', 'too', 'ma', 'some', \"wasn't\", 'up', 'there', \"didn't\", 'in', 'or', \"you've\", 'y', 'what', 'itself', \"wouldn't\", \"shouldn't\", 'then', 'through', 'were', 'those', 'but', \"it's\", 'ain', 'only', 're', 'at', 'didn', 'about', \"shan't\", 'won', 'how', 'whom', 'me', 'between', 'have', 'not', 'ourselves', 'for', 't', 'now', \"that'll\", 'of', \"don't\", 'll', 'which', 'this', 'both', 'herself', 'theirs', 'did', 'before', \"you're\", 'don', \"haven't\", 'will', 'where', 'if', 'are', 'an', 'i', 'into', 'after', 'doing', 'further', 'no'}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed7c5b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'play', 'makes', 'jack', 'dull', 'boy', ',', 'work', 'play']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in tokens if word not in stopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5b9549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926aeda",
   "metadata": {},
   "source": [
    "### Стемминг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41b56480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "words = [\"game\", \"gaming\", \"gamed\", \"games\", \"compacted\"]\n",
    "words_ru = ['корова', 'мальчики', 'мужчины', 'столом', 'убежала']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9247efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['game', 'game', 'game', 'game', 'compact']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "list(map(ps.stem, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07e4e1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['коров', 'мальчик', 'мужчин', 'стол', 'убежа']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Более современная - усовершенствованная\n",
    "ss = SnowballStemmer(language='russian')\n",
    "list(map(ss.stem, words_ru))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f4c4e",
   "metadata": {},
   "source": [
    "### Лемматизация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe3fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "raw_ru = \"\"\"Не существует научных доказательств в пользу эффективности НЛП, оно \n",
    "признано псевдонаукой. Систематические обзоры указывают, что НЛП основано на \n",
    "устаревших представлениях об устройстве мозга, несовместимо с современной \n",
    "неврологией и содержит ряд фактических ошибок.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c4a53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не существовать научный доказательство в польза эффективность нлп, оно \n",
      "признать псевдонаукой. систематический обзор указывают, что нлп основать на \n",
      "устаревший представление о устройство мозга, несовместимый с современный \n",
      "неврология и содержать ряд фактический ошибок.\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "#Работает чисто на слове (ru base подход)\n",
    "'''\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "pymorphy_results = list(map(lambda x: morph.parse(x)[0].normal_form, raw_ru.split(' ')))\n",
    "print(' '.join(pymorphy_results))\n",
    "'''\n",
    "# 1\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "pymorphy_results = list(map(lambda x: morph.parse(x), raw_ru.split(' ')))\n",
    "print(' '.join([res[0].normal_form for res in pymorphy_results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "551d494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "DENNIS : listen , strange woman lie in pond distribute sword \n",
      " be no basis for a system of government .   Supreme executive power derive from \n",
      " a mandate from the masse , not from some farcical aquatic ceremony .\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "#Работает Контекстно!!! нейросетевой подход\n",
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "download(model=\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_results = nlp(raw)\n",
    "print(' '.join([token.lemma_ for token in spacy_results]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651be567",
   "metadata": {},
   "source": [
    "Сравнение PyMorphy2 и PyMystem3  - https://habr.com/ru/articles/503420/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca888471",
   "metadata": {},
   "source": [
    "### Part-of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9efb008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', OpencorporaTag('PRCL')),\n",
       " ('существовать', OpencorporaTag('VERB,impf,intr sing,3per,pres,indc')),\n",
       " ('научный', OpencorporaTag('ADJF,Qual plur,gent')),\n",
       " ('доказательство', OpencorporaTag('NOUN,inan,neut plur,gent')),\n",
       " ('в', OpencorporaTag('PREP')),\n",
       " ('польза', OpencorporaTag('NOUN,inan,femn sing,accs')),\n",
       " ('эффективность', OpencorporaTag('NOUN,inan,femn sing,gent')),\n",
       " ('нлп,', OpencorporaTag('UNKN')),\n",
       " ('оно', OpencorporaTag('NPRO,neut,3per,Anph sing,nomn'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 части речи там и т.д.\n",
    "[(res[0].normal_form, res[0].tag) for res in pymorphy_results[:9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca1d01f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DENNIS', 'PROPN'),\n",
       " (':', 'PUNCT'),\n",
       " ('listen', 'VERB'),\n",
       " (',', 'PUNCT'),\n",
       " ('strange', 'ADJ'),\n",
       " ('woman', 'NOUN'),\n",
       " ('lie', 'VERB')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "[(token.lemma_, token.pos_) for token in spacy_results[:7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b19ddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer LSTM_1_forward will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM_1_backward will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM_0 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM_0 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM_0 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('не', 'PART', '_'),\n",
       " ('существовать',\n",
       "  'VERB',\n",
       "  'Mood=Ind|Number=Sing|Person=3|Tense=Notpast|VerbForm=Fin|Voice=Act'),\n",
       " ('научный', 'ADJ', 'Case=Gen|Degree=Pos|Number=Plur'),\n",
       " ('доказательство', 'NOUN', 'Case=Gen|Gender=Neut|Number=Plur'),\n",
       " ('в', 'ADP', '_'),\n",
       " ('польза', 'NOUN', 'Case=Acc|Gender=Fem|Number=Sing'),\n",
       " ('эффективность', 'NOUN', 'Case=Gen|Gender=Fem|Number=Sing')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3\n",
    "#нейросетевой подход на основе рекуррентных нейронок\n",
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "rnnmorph_result = predictor.predict(raw_ru.split(' '))\n",
    "[(token.normal_form, token.pos, token.tag) for token in rnnmorph_result[:7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db766a0",
   "metadata": {},
   "source": [
    "## Задача: Named entities recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55291194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e0bf1",
   "metadata": {},
   "source": [
    "### Задача классификации\n",
    "\n",
    "##### 20 newsgroups\n",
    "\n",
    "##### Датасет с 18000 новостей, сгруппированных по 20 темам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8428e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16f28251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b145601e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc36e028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Рассмотрим только 4 темы:\n",
    "categories = ['alt.atheism', 'talk.religion.misc',\n",
    "              'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      categories=categories)\n",
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d1fec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rych@festival.ed.ac.uk (R Hawkes)\n",
      "Subject: 3DS: Where did all the texture rules go?\n",
      "Lines: 21\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "======================================================================\n",
      "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
      "Virtual Environment Laboratory\n",
      "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
      "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1901ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce12953",
   "metadata": {},
   "source": [
    "#### TF-IDF(напоминание)\n",
    "\n",
    "$n_{\\mathbb{d}\\mathbb{w}}$ - число вхождений слова $\\mathbb{w}$ в документ $\\mathbb{d}$;<br>\n",
    "$N_{\\mathbb{w}}$ - число документов, содержащих $\\mathbb{w}$;<br>\n",
    "$N$ - число документов; <br><br>\n",
    "\n",
    "$p(\\mathbb{w}, \\mathbb{d}) = N_{\\mathbb{w}} / N$ - вероятность наличия слова $\\mathbb{w}$ в любом документе $\\mathbb{d}$\n",
    "<br>\n",
    "$P(\\mathbb{w}, \\mathbb{d}, n_{\\mathbb{d}\\mathbb{w}}) = (N_{\\mathbb{w}} / N)^{n_{\\mathbb{d}\\mathbb{w}}}$ - вероятность встретить $n_{\\mathbb{d}\\mathbb{w}}$ раз слово $\\mathbb{w}$ в документе $\\mathbb{d}$<br><br>\n",
    "\n",
    "$-\\log{P(\\mathbb{w}, \\mathbb{d}, n_{\\mathbb{d}\\mathbb{w}})} = n_{\\mathbb{d}\\mathbb{w}} \\cdot \\log{(N / N_{\\mathbb{w}})} = TF(\\mathbb{w}, \\mathbb{d}) \\cdot IDF(\\mathbb{w})$<br><br>\n",
    "\n",
    "$TF(\\mathbb{w}, \\mathbb{d}) = n_{\\mathbb{d}\\mathbb{w}}$ - term frequency;<br>\n",
    "$IDF(\\mathbb{w}) = \\log{(N /N_{\\mathbb{w}})}$ - inverted document frequency;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1205678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed44149",
   "metadata": {},
   "source": [
    "#### Некоторые параметры: \n",
    "* input : string {‘filename’, ‘file’, ‘content’}\n",
    "*  lowercase : boolean, default True\n",
    "*  preprocessor : callable or None (default)\n",
    "*  tokenizer : callable or None (default)\n",
    "*  stop_words : string {‘english’}, list, or None (default)\n",
    "*  ngram_range : tuple (min_n, max_n)\n",
    "*  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "*  min_df : float in range [0.0, 1.0] or int, default=1\n",
    "*  max_features : int or None, default=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03bfd151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 797 ms\n",
      "Wall time: 916 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2034, 34118)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# lowercase\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abf33f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.03 s\n",
      "Wall time: 1.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2034, 42307)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03715705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', '00000', '000000', '000005102000', '000021',\n",
       "       '000062David42', '0000VEC', '0001'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:10]\n",
    "#странные слова, хотим ограничить вероятности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be65f854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min_df, max_df\n",
    "vectorizer = TfidfVectorizer(min_df=0.8)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aa8d7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Николай\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and', 'from', 'in', 'lines', 'of', 'organization', 'subject', 'the', 'to']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73d4fd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 719 ms\n",
      "Wall time: 731 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2034, 2391)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.8)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db48cea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.55 s\n",
      "Wall time: 3.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2034, 1236)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# ngram_range - взяимодействие токенов\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=0.03, max_df=0.9)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11188cab",
   "metadata": {},
   "source": [
    "### Лемматизация в nltk + Tfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3da874a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Николай\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# стоп-слова, preproc\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "#nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9820c100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh , think landed miracle work , thirst hunger come conference bird'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Совсем простой лемматизатор \n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preproc_nltk(text):\n",
    "    #text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "    return ' '.join([wnl.lemmatize(word) for word in word_tokenize(text.lower()) if word not in stopWords])\n",
    "\n",
    "st = \"Oh, I think I ve landed Where there are miracles at work,  For the thirst and for the hunger Come the conference of birds\"\n",
    "preproc_nltk(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9885d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.8 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(preprocessor=preproc_nltk)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc581f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh , I think I land miracle work ,   thirst hunger come conference bird'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Тоже самое со Spacy:\n",
    "# preproc_spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "texts = newsgroups_train.data.copy()\n",
    "\n",
    "def preproc_spacy(text):\n",
    "    spacy_results = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in spacy_results if token.lemma_ not in stopWords])\n",
    "preproc_spacy(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad4a8790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%time\\nnew_texts = []\\nfor doc in nlp.pipe(texts, batch_size=64, n_process=3, disable=[\"parser\", \"ner\"]):\\n    new_texts.append(\\' \\'.join([tok.lemma_ for tok in doc if tok.lemma not in stopWords]))\\nvectorizer = TfidfVectorizer()\\nvectors = vectorizer.fit_transform(new_texts)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''%%time\n",
    "new_texts = []\n",
    "for doc in nlp.pipe(texts, batch_size=64, n_process=3, disable=[\"parser\", \"ner\"]):\n",
    "    new_texts.append(' '.join([tok.lemma_ for tok in doc if tok.lemma not in stopWords]))\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(new_texts)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec025808",
   "metadata": {},
   "source": [
    "### Итоговая модель:\n",
    " с помощью nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4c8640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.9 s\n",
      "Wall time: 14.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " 'area',\n",
       " 'color',\n",
       " 'edu keith allan',\n",
       " 'halat',\n",
       " 'last',\n",
       " 'nature',\n",
       " 'prb access digex',\n",
       " 'sgi',\n",
       " 'told']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(preprocessor=preproc_nltk, ngram_range=(1, 3), max_df=0.5, max_features=1000)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectorizer.get_feature_names()[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5963e5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 1000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d11eb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = vectors.todense()[0]\n",
    "(vector != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32257b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.39773844641101"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(list(map(lambda x: (x != 0).sum(), vectors.todense())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c95f06cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2dbabcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 1000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_vectors = vectors.todense()\n",
    "dense_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c9d0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1, v2):\n",
    "    # v1, v2 (1 x dim)\n",
    "    return np.array(v1 @ v2.T / norm(v1) / norm(v2))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71b2f680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.012832470287321068,\n",
       " 0.0,\n",
       " 0.029017963562689052,\n",
       " 0.05098769169092632,\n",
       " 0.013113293419448975,\n",
       " 0.02985566737595214,\n",
       " 0.22451261871125824,\n",
       " 0.024460499336962052,\n",
       " 0.018675832581253434]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosines = []\n",
    "for i in range(10):\n",
    "    cosines.append(cosine_sim(dense_vectors[0], dense_vectors[i]))\n",
    "cosines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33f56f",
   "metadata": {},
   "source": [
    "### Обучим любую известную модель на полученных признаках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "321f50a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1627,), (407,))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(dense_vectors, newsgroups_train.target, test_size=0.2, random_state=0)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6495ffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.14 s\n",
      "Wall time: 2.11 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svc = svm.SVC()\n",
    "svc.fit(np.asarray(X_train), np.asarray(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1b7cf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9385749385749386"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.asarray(y_test), svc.predict(np.asarray((X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9054f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Николай\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "C:\\Users\\Николай\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train)\n",
    "accuracy_score(y_test, sgd.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9eaede",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bdcc0ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "embeddings_pretrained = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a4ed46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "proc_words = [preproc_nltk(text).split() for text in newsgroups_train.data]\n",
    "embeddings_trained = Word2Vec(proc_words, # data for model to train on\n",
    "                 vector_size=100,                 # embedding vector size\n",
    "                 min_count=3,             # consider words that occured at least 5 times\n",
    "                 window=3).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a79704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sum(comment, embeddings):\n",
    "    \"\"\"\n",
    "    implement a function that converts preprocessed comment to a sum of token vectors\n",
    "    \"\"\"\n",
    "    embedding_dim = embeddings.vectors.shape[1]\n",
    "    features = np.zeros([embedding_dim], dtype='float32')\n",
    "    #i = 0\n",
    "    for word in preproc_nltk(comment).split():\n",
    "        if word in embeddings:\n",
    "            #i+=1\n",
    "            features += embeddings[f'{word}']\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e15d8993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13566"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_trained.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "acff2d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1627, 25), (407, 25))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wv = np.stack([vectorize_sum(text, embeddings_pretrained) for text in newsgroups_train.data])\n",
    "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, newsgroups_train.target, test_size=0.2, random_state=0)\n",
    "X_train_wv.shape, X_test_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61b824a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027027027027027"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7cb4013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1627, 100), (407, 100))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wv = np.stack([vectorize_sum(text, embeddings_trained) for text in newsgroups_train.data])\n",
    "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, newsgroups_train.target, test_size=0.2, random_state=0)\n",
    "X_train_wv.shape, X_test_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "80294e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Николай\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8427518427518428"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b559971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
